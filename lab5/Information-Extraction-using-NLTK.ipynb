{
 "metadata": {
  "name": "",
  "signature": "sha256:496a266278080871c195f98db07bbdb327b2b8deec9777f15b228dada101f5a6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 2: Information Extraction using NLTK\n",
      "\n",
      "[NLTK](http://www.nltk.org/) is a python platform for natural language processing and information extraction. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, and an active discussion forum.\n",
      "\n",
      "The associated book, [Natural Language Processing with Python](http://www.nltk.org/book/) is also freely available and is a great resource both for NLP concepts and for practical examples of how to use the NLTK package.\n",
      "\n",
      "To get started, you need to install NLTK:\n",
      "    \n",
      "    sudo pip install -U nltk\n",
      "    \n",
      "After running the first command below (download()), you will be presented with a window to select which 'data' to download. For space reasons, only download 'book' (Everything used in the NLTK book). This in itself is a few hundred MB download."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test it is working\n",
      "\n",
      "sentence = \"\"\"At eight o'clock on Thursday morning, Arthur didn't feel very good.\"\"\"\n",
      "tokens = nltk.word_tokenize(sentence)\n",
      "tagged = nltk.pos_tag(tokens)\n",
      "tagged"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "[('At', 'IN'),\n",
        " ('eight', 'CD'),\n",
        " (\"o'clock\", 'JJ'),\n",
        " ('on', 'IN'),\n",
        " ('Thursday', 'NNP'),\n",
        " ('morning', 'NN'),\n",
        " (',', ','),\n",
        " ('Arthur', 'NNP'),\n",
        " ('did', 'VBD'),\n",
        " (\"n't\", 'RB'),\n",
        " ('feel', 'VB'),\n",
        " ('very', 'RB'),\n",
        " ('good', 'JJ'),\n",
        " ('.', '.')]"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above two commands tokenize the string, and tag each of the token with the part-of-speech. Here is a listing of tagsets that NLTK uses here (there are different tagsets used by different corpora).\n",
      "http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following set of commands extracts named entities."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entities = nltk.chunk.ne_chunk(tagged)\n",
      "entities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), (',', ','), Tree('PERSON', [('Arthur', 'NNP')]), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')])"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Visual representation of the tree \n",
      "entities.draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Named Entity Recognition \n",
      "\n",
      "As a somewhat more elaborate example, the following sequence of commands reads data from a file, and does NER on each of the sentences in the file. It doesn't do a very good job on this article, but in general, it seems to work quite well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"news1.html\", \"r\") as myfile:\n",
      "    data = myfile.read()   \n",
      "sentences = nltk.sent_tokenize(data)\n",
      "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
      "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
      "print(nltk.ne_chunk(sentences[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'news1.html'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-32-2f2e3a95f721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"news1.html\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'news1.html'"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Relation Extraction\n",
      "\n",
      "The second key task is to extract relations between entities. The following code snippet finds the relations between organizations and locations, in one of the existing datasets in NLTK. See the book webpage for more details on the regular expression pattern below: http://www.nltk.org/book/ch07.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
      "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
      "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n",
      "        print(nltk.sem.rtuple(rel))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Assignment \n",
      "\n",
      "**PART 2.1 (NER):** Download, say 10 recent news articles, on some topic. Write code to extract named entities from each of them. The final output should simply be a list of entities and their types, which would require understanding the structure of the output of the ne_chunk command, and traversing it to find just the named entities. \n",
      "\n",
      "Submit both the python code, and the entities you extracted. For example, for the article above, the output should be:\n",
      "\n",
      "    Ebola, PERSON\n",
      "    Published, PERSON\n",
      "    Facebook, PERSON\n",
      "    ...\n",
      "\n",
      "**Part 2.2 (Relation Extraction):** Write a few regular expressions to extract different types of PERSON-ORGANIZATION relationships (e.g., PERSON executive at ORGANIZATION) over the same dataset (the IEER Corpus). You can use the above script mostly unchanged with the changes being: definition of the pattern IN, and the arguments to `extract_rels`. \n",
      "\n",
      "It may be useful to see the text of some of the documents, e.g., the second document in the above corpus can be seen by running:\n",
      "\n",
      "    print(nltk.corpus.ieer.parsed_docs('NYT_19980315')[1].text)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import glob\n",
      "import os\n",
      "import codecs\n",
      "\n",
      "NE_TYPES = ['ORGANIZATION', 'PERSON','LOCATION','DATE','TIME','MONEY','PERCENT','FACILITY','GPE']\n",
      "\n",
      "\"\"\"\n",
      "def traverse(t):\n",
      "    if not t or not t.label():\n",
      "        return\n",
      "    else:\n",
      "        if t == 'PERSON': \n",
      "            print(t.label()) # or do something else\n",
      "        for child in t:\n",
      "            traverse(child)\n",
      "\"\"\"\n",
      "\n",
      "def traverse(t):\n",
      "    try:\n",
      "        t.label()\n",
      "    except AttributeError:\n",
      "        pass\n",
      "    else:\n",
      "        # Now we know that t.node is defined\n",
      "        if t.label() in NE_TYPES:\n",
      "            lbl = t.label()\n",
      "            s = \"\"\n",
      "            for l in t.leaves():\n",
      "                s = s + \" \" + l[0]\n",
      "            print(s + \", \" + lbl)\n",
      "        for child in t:\n",
      "            traverse(child)\n",
      "\n",
      "articles = []\n",
      "\n",
      "for file in glob.glob(\"./news/*.html\"):\n",
      "    with codecs.open(file, \"r\",\"utf-8\") as myfile:\n",
      "        articles = articles + [myfile.read()]\n",
      "\n",
      "article = articles[0]\n",
      "sentences = nltk.sent_tokenize(article)\n",
      "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
      "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
      "\n",
      "i = 0\n",
      "max_len = len(sentences)\n",
      "while(i < max_len):\n",
      "    traverse(nltk.ne_chunk(sentences[i]))\n",
      "    i+=1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Holder, GPE\n",
        " Fox News, PERSON\n",
        " Published, PERSON\n",
        " Eric Holder, PERSON\n",
        " Washington, GPE\n",
        " AP, ORGANIZATION\n",
        " Eric Holder, PERSON\n",
        " Fox News, PERSON\n",
        " James Rosen, PERSON\n",
        " Washington, GPE\n",
        " Holder, PERSON\n",
        " Fox, PERSON\n",
        " Rosen, PERSON\n",
        " Holder, PERSON\n",
        " Rosen, PERSON\n",
        " Justice Department, ORGANIZATION"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Rosen, PERSON\n",
        " North Korea, GPE\n",
        " Rosen, PERSON\n",
        " FBI, ORGANIZATION\n",
        " Rosen, PERSON\n",
        " Espionage, ORGANIZATION\n",
        " Holder, PERSON\n",
        " Holder, PERSON"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Rosen, GPE\n",
        " Washington, GPE\n",
        " Fox News, PERSON\n",
        " Fox News, PERSON"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Roger Ailes, PERSON\n",
        " Rosen, PERSON\n",
        " Suffice, GPE\n",
        " Rosen, PERSON"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Justice Department, ORGANIZATION\n",
        " Holder, PERSON\n",
        " House, ORGANIZATION\n",
        " Holder, PERSON\n",
        " Congress, ORGANIZATION\n",
        " Justice Department, ORGANIZATION\n",
        " Associated, ORGANIZATION\n",
        " Holder, GPE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Obama, PERSON\n",
        " DOJ, ORGANIZATION\n",
        " Congress, ORGANIZATION"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Congress, ORGANIZATION\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}